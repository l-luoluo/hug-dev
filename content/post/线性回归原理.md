## 线性回归原理



![](https://gitee.com/L-luoluo/picgo-store-imgages/raw/master/picgo/图1.png)

​	在上述式子中的w可以理解为一个事物的特征，而f(x)可以理解为这个事物，就像是人有眼睛，鼻子，耳朵，嘴巴，这些就是作为人的特征，我们学习是通过学习事物的特征来进行识别这个事物是什么，而机器学习则不同；我们将一组人的图扔给机器，告诉它这个是人，然后它会通过这些图片自己总结作为人的特征，也就是上述式子中的参数w和b，最后随便给机器一张图它就能判断这是不是人



## 线性回归案例

![](https://gitee.com/L-luoluo/picgo-store-imgages/raw/master/picgo/图2.png)

## 损失函数

![](https://gitee.com/L-luoluo/picgo-store-imgages/raw/master/picgo/图3.png)

​	关于损失函数我只这样理解的，一组数据会有特别多，并不是说所有的数据对应的点都会在我们规定这个函数对应的函数曲线图谱上，而我们只是尽可能让这条曲线尽可能穿过更多的点，但这时我们该如何判断我们的曲线比较符合这组数据，这时就需要用到损失函数。损失函数顾名思义就是真实值与预测值之间的误差，损失值越小，模型的拟合结果越好



## 损失函数求解——穷举法

![](https://gitee.com/L-luoluo/picgo-store-imgages/raw/master/picgo/图4.png)

穷举法，顾名思义就是将所有可能出现的结果一 一列举出来，然后再判断最优的一组参数，但是当可能出现的结果有无数个时，我们没办法去一 一列举出来，这时就需要用到最小二乘法



## 损失函数求解——最小二乘法

![](https://gitee.com/L-luoluo/picgo-store-imgages/raw/master/picgo/图5.png)

​	最小二乘法顾名思义就是去最小值的一个方法，用此方法我们可以求出loss最小值，这时对应参数就是最优解

![](https://gitee.com/L-luoluo/picgo-store-imgages/raw/master/picgo/图6.png)

​	我们前面说过了x(特征)可不止一个，所以我们又可以将x转换为一个矩阵即w,方便进行运算，并且我们同时也能将b也写入矩阵

![](https://gitee.com/L-luoluo/picgo-store-imgages/raw/master/picgo/图7.png)
![](https://gitee.com/L-luoluo/picgo-store-imgages/raw/master/picgo/图8.png)

​	这个就是矩阵W的整个求解过程



